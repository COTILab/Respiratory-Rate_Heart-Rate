%kcbpnn_train 'Create and train a Back-Propagation Neural Network '
% This MatLab function was automatically generated by a converter (KhorosToMatLab) from the Khoros cbpnn_train.pane file
%
% Parameters: 
% InputFile: itrain 'Input Training Patterns', required: 'input training patterns file '
% Integer: hiddenlayers 'Number of hidden layers', default: 1: 'the number of hidden layers for this model (1-5)'
% String: unitslayer 'Number of units/hidden layer (separated by spaces)', default: '3': 'string with the number of units/layer separated by spaces'
% Integer: maxiterations 'Max number of iterations', default: 100: 'number of times the net will be trained'
% Integer: epoch 'Epoch size', default: 100: 'size of epoch '
% OutputFile: o 'Output network', required: 'Resulting output data object (nn model)'
% OutputFile: oinfo 'Output Information File', optional: 'output file for details on the training of the network (debug)'
% Toggle: details 'Sample Details', default: 0: 'if set will print information about the samples too'
%
% Example: [o, oinfo] = kcbpnn_train(itrain, {'itrain','';'hiddenlayers',1;'unitslayer','3';'maxiterations',100;'epoch',100;'o','';'oinfo','';'details',0})
%
% Khoros helpfile follows below:
%
%  PROGRAM
% cbpnn_train - Create and train a Back-Propagation Neural Network
%
%  DESCRIPTION
% This routine creates and train a Back-Propagation Neural Network that will be used for classification with the trained information. 
% Training is done by presenting the input data to the network, calculating its outputs, comparing the outputs with the expected results and adjusting the weights of the network. In this process, the network will hopefully converge or "learn" how to discriminate the different inputs so when they are presented the expected results (or values close to those) will be achieved. Each cycle of presenting the input, calculating the output, comparing results and changing weights is an "epoch".
% Training a network can take a lot of time, and there is no guarantee that it will converge. If the network does not converge it means that classification results can be unsatisfactory. Convergence is controlled by several factors, and there is no simple way to determine whether an architecture or the chosen parameters will perform satisfactorily (or even converge) for the data you're using. Your best bet is to divide the training data in two sets, train the network with one until the results are acceptable and then execute the network on the test data to see whether the network is performing correctly. This technique is described in the Classify Toolbox manual.
% Before training the network we need to specify its architecture - i.e. how many layers it will have and how many units (neurons) in each layer. For the model used in this toolbox, the input and output layer sizes are predefined - the input layer size will be the number of features in the data and the output layer size will be the number of classes. The user must provide a number of hidden layers with the parameter [-hiddenlayers] and the number of units in each of this layer with the [-unitslayer] parameter. The [-unitslayer] parameter will consist of N integer values separated by spaces where N is the number of hidden layers. For example, to specify a network with 2 hidden layers with 10 units in the first and 4 in the second hidden layers, use [-hiddenlayers 2] and [-unitslayer '10 4'].
% The data that will be used to train the network must be a labeled data set that can be obtained by creating the signatures with the "cbpnn_signature" program and joining/labeling them with the \fIcsigappend\fP program. The input labeled samples are passed with the [-itrain] parameter. Do \fBnot\fP use masked data or simple ROIs here, since the input data must be scanned by the network several times, the checking of mask values will take additional time. Use always the outputs of the \fIcbpnn_signature\fP/\fIcsigappend\fP programs.
% The result of this program is the set of trained weights that connect the network and the network architecture. This data will be saved in the file specified with the [-o] parameter, for use by "cbpnn_classify".
% The network will be trained until one of these three conditions occurs:
% The number of epochs exceed the maximum number passed by the [-maxiterations] parameter or
% The total training error (difference between the results obtained by the network with the expected results) is smaller than the specified total error passed by the parameter [-maxtotalerror] or
% The maximum individual error for a sample is smaller than the specified value passed by the parameter [-maxinderror].
% To speed the process of training, the stopping conditions are checked only after each epoch, but each epoch actually can consist of several runs through the cycle of presenting the input, calculating the output, comparing results and changing weights. The number of cycles that will be considered an epoch is determined by the parameter [-epoch]. A low value (20-100) will not slow the training too much, and can serve to inspect whether the network is really converging (if you're using the parameter [-oinfo] to check the network). 
% Two other parameters control the speed of the network convergence: The momentum rate (passed with the [-alpha] parameter), which will determine the change rate in the weights: if the changes are large (meaning that the network is far from convergence) the weight changes will be large, if the changes are small (meaning that the network is close to converge) the changes will be small; and the learning rate Epsilon (passed with the [-epsilon] parameter), which will control the convergence of the network, and it is usually << 1 making the network take smaller steps towards the solution.  
% The network expected output is created as a binary index for the class. For example, if we want the expected result for a sample to be class 5 on a 8-class problem, its expected output will be internally represented by the vector (0,0,0,1,0,0,0,0), and the network will be trained to produce a result as close as possible to this one. The problem is that the network won't be able to produce results close to zero or one, so we must give approximations if we want the individual and total errors to be smaller and the network to converge. For example, we can select one to be actually 0.9 and zero to be actually 0.1 - these are the activation and non-activation values, passed respectively by the parameters [-one] and [-zero]. 
% The convergence of the network can be verified while the network is being trained. If a file name is passed with the [-oinfo] parameter, information about the convergence, weights and epochs will be written in ASCII to that file. If the parameter [-details] is passed, the file will contain information about the activations for each sample, but the file can become very large for training with several samples.
% .SH "Suggestions for training the network:"
% Here are some suggestions to try when the network isn't apparently converging or it is converging too slowly. Keep in mind that the training is \fBvery" dependend on the data and the parameter, these are only some basic guidelines.
% * Try several variations of the number of hidden layers and units in the layers. Keep in mind that too many units and layers will slow down the training, and there is no reason to use more than two layers. On the other hand, too few neurons will not be able to train the network.  
% * Setting the [-epsilon] parameter to values closer to one can improve the convergence in simple cases.
% * Values for [-one] and [-zero] more distant from the extremes (e.g. 0.7 for [-one] and 0.3 for [-zero]) can make convergence faster but can lead to poor classification results.
% * The initial weights are random, meaning that there is no guarantee that two runs of the network with identical data will perform similarly.
% * Use the "cbpnn_sigcheck" program to see if the classes are really separable - the network will \fBnever\fP converge if two equal samples are assigned to different classes before training.
% This program is based on a Back-Propagation Neural Network program by Prof. Takeshi Ohashi of the Ejima Lab, AI Dept., Kyushu Institute of Technology, Iizuka, Japan.
%
%  
%
%  EXAMPLES
% All examples for the Classify toolbox are listed on the Classify Toolbox Manual. For an example of this routine, please see the Classify:workspaces:BPNN-Image-Train or Classify:workspaces:BPNN-Image example workspaces.
%
%  "SEE ALSO"
% cbpnn_signature, cbpnn_classify, cbpnn_sigcheck, csigappend.
%
%  RESTRICTIONS 
%
%  REFERENCES 
% All references for the Classify toolbox are listed on the Classify Toolbox Manual.
%
%  COPYRIGHT
% Copyright (C) 1997 Rafael Santos. Khoros (C) Khoral Research, Inc.
% 


function varargout = kcbpnn_train(varargin)
if nargin ==0
  Inputs={};arglist={'',''};
elseif nargin ==1
  Inputs=varargin{1};arglist={'',''};
elseif nargin ==2
  Inputs=varargin{1}; arglist=varargin{2};
else error('Usage: [out1,..] = kcbpnn_train(Inputs,arglist).');
end
if size(arglist,2)~=2
  error('arglist must be of form {''ParameterTag1'',value1;''ParameterTag2'',value2}')
 end
narglist={'itrain', '__input';'hiddenlayers', 1;'unitslayer', '3';'maxiterations', 100;'epoch', 100;'o', '__output';'oinfo', '__output';'details', 0};
maxval={0,5,0,2,2,0,1,0};
minval={0,1,0,2,2,0,1,0};
istoggle=[0,0,0,0,0,0,1,1];
was_set=istoggle * 0;
paramtype={'InputFile','Integer','String','Integer','Integer','OutputFile','OutputFile','Toggle'};
% identify the input arrays and assign them to the arguments as stated by the user
if ~iscell(Inputs)
Inputs = {Inputs};
end
NumReqOutputs=1; nextinput=1; nextoutput=1;
  for ii=1:size(arglist,1)
  wasmatched=0;
  for jj=1:size(narglist,1)
   if strcmp(arglist{ii,1},narglist{jj,1})  % a given argument was matched to the possible arguments
     wasmatched = 1;
     was_set(jj) = 1;
     if strcmp(narglist{jj,2}, '__input')
      if (nextinput > length(Inputs)) 
        error(['Input ' narglist{jj,1} ' has no corresponding input!']); 
      end
      narglist{jj,2} = 'OK_in';
      nextinput = nextinput + 1;
     elseif strcmp(narglist{jj,2}, '__output')
      if (nextoutput > nargout) 
        error(['Output nr. ' narglist{jj,1} ' is not present in the assignment list of outputs !']); 
      end
      if (isempty(arglist{ii,2}))
        narglist{jj,2} = 'OK_out';
      else
        narglist{jj,2} = arglist{ii,2};
      end

      nextoutput = nextoutput + 1;
      if (minval{jj} == 0)  
         NumReqOutputs = NumReqOutputs - 1;
      end
     elseif isstr(arglist{ii,2})
      narglist{jj,2} = arglist{ii,2};
     else
        if strcmp(paramtype{jj}, 'Integer') & (round(arglist{ii,2}) ~= arglist{ii,2})
            error(['Argument ' arglist{ii,1} ' is of integer type but non-integer number ' arglist{ii,2} ' was supplied']);
        end
        if (minval{jj} ~= 0 | maxval{jj} ~= 0)
          if (minval{jj} == 1 & maxval{jj} == 1 & arglist{ii,2} < 0)
            error(['Argument ' arglist{ii,1} ' must be bigger or equal to zero!']);
          elseif (minval{jj} == -1 & maxval{jj} == -1 & arglist{ii,2} > 0)
            error(['Argument ' arglist{ii,1} ' must be smaller or equal to zero!']);
          elseif (minval{jj} == 2 & maxval{jj} == 2 & arglist{ii,2} <= 0)
            error(['Argument ' arglist{ii,1} ' must be bigger than zero!']);
          elseif (minval{jj} == -2 & maxval{jj} == -2 & arglist{ii,2} >= 0)
            error(['Argument ' arglist{ii,1} ' must be smaller than zero!']);
          elseif (minval{jj} ~= maxval{jj} & arglist{ii,2} < minval{jj})
            error(['Argument ' arglist{ii,1} ' must be bigger than ' num2str(minval{jj})]);
          elseif (minval{jj} ~= maxval{jj} & arglist{ii,2} > maxval{jj})
            error(['Argument ' arglist{ii,1} ' must be smaller than ' num2str(maxval{jj})]);
          end
        end
     end
     if ~strcmp(narglist{jj,2},'OK_out') &  ~strcmp(narglist{jj,2},'OK_in') 
       narglist{jj,2} = arglist{ii,2};
     end
   end
   end
   if (wasmatched == 0 & ~strcmp(arglist{ii,1},''))
        error(['Argument ' arglist{ii,1} ' is not a valid argument for this function']);
   end
end
% match the remaining inputs/outputs to the unused arguments and test for missing required inputs
 for jj=1:size(narglist,1)
     if  strcmp(paramtype{jj}, 'Toggle')
        if (narglist{jj,2} ==0)
          narglist{jj,1} = ''; 
        end;
        narglist{jj,2} = ''; 
     end;
     if  ~strcmp(narglist{jj,2},'__input') && ~strcmp(narglist{jj,2},'__output') && istoggle(jj) && ~ was_set(jj)
          narglist{jj,1} = ''; 
          narglist{jj,2} = ''; 
     end;
     if strcmp(narglist{jj,2}, '__input')
      if (minval{jj} == 0)  % meaning this input is required
        if (nextinput > size(Inputs)) 
           error(['Required input ' narglist{jj,1} ' has no corresponding input in the list!']); 
        else
          narglist{jj,2} = 'OK_in';
          nextinput = nextinput + 1;
        end
      else  % this is an optional input
        if (nextinput <= length(Inputs)) 
          narglist{jj,2} = 'OK_in';
          nextinput = nextinput + 1;
        else 
          narglist{jj,1} = '';
          narglist{jj,2} = '';
        end;
      end;
     else 
     if strcmp(narglist{jj,2}, '__output')
      if (minval{jj} == 0) % this is a required output
        if (nextoutput > nargout & nargout > 1) 
           error(['Required output ' narglist{jj,1} ' is not stated in the assignment list!']); 
        else
          narglist{jj,2} = 'OK_out';
          nextoutput = nextoutput + 1;
          NumReqOutputs = NumReqOutputs-1;
        end
      else % this is an optional output
        if (nargout - nextoutput >= NumReqOutputs) 
          narglist{jj,2} = 'OK_out';
          nextoutput = nextoutput + 1;
        else 
          narglist{jj,1} = '';
          narglist{jj,2} = '';
        end;
      end
     end
  end
end
if nargout
   varargout = cell(1,nargout);
else
  varargout = cell(1,1);
end
global KhorosRoot
if exist('KhorosRoot') && ~isempty(KhorosRoot)
w=['"' KhorosRoot];
else
if ispc
  w='"C:\Program Files\dip\khorosBin\';
else
[s,w] = system('which cantata');
w=['"' w(1:end-8)];
end
end
[varargout{:}]=callKhoros([w 'cbpnn_train"  '],Inputs,narglist);
